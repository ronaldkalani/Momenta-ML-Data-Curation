{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "cb3bf80fe3d646cebc70a0ec69ff82eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3cd467769a23412989e8a6ec62772a60",
              "IPY_MODEL_60f7e78691db4164a2db817fa6c9a4c5",
              "IPY_MODEL_f9c7f08d4a8b488bbc395e9976d73734"
            ],
            "layout": "IPY_MODEL_af6b83af31f54ad3afa1a18c0075954e"
          }
        },
        "3cd467769a23412989e8a6ec62772a60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e8dc70f3a984bf79a53935d7dbad8bf",
            "placeholder": "​",
            "style": "IPY_MODEL_febe4b5e14234ef2ae882bb09112a0d0",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "60f7e78691db4164a2db817fa6c9a4c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_87e441094d184016bbccb37f96443d7b",
            "max": 1269737156,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3e4f24cd6ac242188849487f313da8a9",
            "value": 1269737156
          }
        },
        "f9c7f08d4a8b488bbc395e9976d73734": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b0e622e471f420dbf73a703fdaf401d",
            "placeholder": "​",
            "style": "IPY_MODEL_528f95bb11624637bcdd68e0c7e0a947",
            "value": " 1.27G/1.27G [00:17&lt;00:00, 111MB/s]"
          }
        },
        "af6b83af31f54ad3afa1a18c0075954e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e8dc70f3a984bf79a53935d7dbad8bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "febe4b5e14234ef2ae882bb09112a0d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "87e441094d184016bbccb37f96443d7b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e4f24cd6ac242188849487f313da8a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3b0e622e471f420dbf73a703fdaf401d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "528f95bb11624637bcdd68e0c7e0a947": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: Mount Google Drive**"
      ],
      "metadata": {
        "id": "HLEPECtlD6jm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "h--p1vabr2a_",
        "outputId": "66f0a274-98cb-4853-cd9c-1602a4680779"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = \"/content/drive/My Drive/ML/\"\n",
        "\n"
      ],
      "metadata": {
        "id": "bkvlKo-UsGOR"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Install Required Libraries**"
      ],
      "metadata": {
        "id": "lI1kuv8RD5kA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install librosa torchaudio transformers torch numpy pandas\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "CMXgb78xsRcs",
        "outputId": "603add13-1ca8-4409-cbac-e2cbd4ed6d11"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (0.11.0)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.49.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.14.1)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.12.2)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.29.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (4.3.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m91.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m66.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m77.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3: Load Dataset Paths**"
      ],
      "metadata": {
        "id": "a9K2xmNlEQQe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Update these paths based on your dataset location\n",
        "train_fake_dir = \"/content/drive/My Drive/ML/training/fake\"\n",
        "train_real_dir = \"/content/drive/My Drive/ML/training/real\"\n",
        "val_fake_dir = \"/content/drive/My Drive/ML/validation/fake\"\n",
        "val_real_dir = \"/content/drive/My Drive/ML/validation/real\"\n",
        "test_fake_dir = \"/content/drive/My Drive/ML/testing/fake\"\n",
        "test_real_dir = \"/content/drive/My Drive/ML/testing/real\"\n"
      ],
      "metadata": {
        "id": "HtwFfsPkETOa"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4: Load and Preprocess Audio Data**"
      ],
      "metadata": {
        "id": "rfNKi-95Ea3N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import numpy as np\n",
        "\n",
        "def load_audio_files(directory, label):\n",
        "    audio_data, labels = [], []\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith(\".wav\"):  # Ensure it's an audio file\n",
        "            filepath = os.path.join(directory, filename)\n",
        "            y, sr = librosa.load(filepath, sr=16000)  # Convert all audio to 16kHz\n",
        "            audio_data.append(y)\n",
        "            labels.append(label)\n",
        "    return audio_data, labels\n",
        "\n",
        "# Load data\n",
        "train_fake_audio, train_fake_labels = load_audio_files(train_fake_dir, 0)\n",
        "train_real_audio, train_real_labels = load_audio_files(train_real_dir, 1)\n",
        "val_fake_audio, val_fake_labels = load_audio_files(val_fake_dir, 0)\n",
        "val_real_audio, val_real_labels = load_audio_files(val_real_dir, 1)\n",
        "test_fake_audio, test_fake_labels = load_audio_files(test_fake_dir, 0)\n",
        "test_real_audio, test_real_labels = load_audio_files(test_real_dir, 1)\n",
        "\n",
        "# Merge datasets\n",
        "X_train, y_train = train_fake_audio + train_real_audio, train_fake_labels + train_real_labels\n",
        "X_val, y_val = val_fake_audio + val_real_audio, val_fake_labels + val_real_labels\n",
        "X_test, y_test = test_fake_audio + test_real_audio, test_fake_labels + test_real_labels\n",
        "\n",
        "print(f\"Loaded {len(X_train)} training samples, {len(X_val)} validation samples, and {len(X_test)} test samples.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "RSB57tSkEe0-",
        "outputId": "20e0db41-1140-4618-8b71-4ca70bb94642"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 13956 training samples, 2826 validation samples, and 1088 test samples.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Purpose:** Loads and preprocesses raw audio waveforms, converting them to 16kHz sampling rate"
      ],
      "metadata": {
        "id": "vbvPCC39EoyZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 5: Load Wav2Vec 2.0 Feature Extractor**"
      ],
      "metadata": {
        "id": "Y40USkQDEx-t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Wav2Vec2FeatureExtractor\n",
        "\n",
        "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/wav2vec2-large-xlsr-53\")\n"
      ],
      "metadata": {
        "id": "rMJukzQ-E22s"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Purpose: Extracts waveform features from raw audio using Wav2Vec 2.0."
      ],
      "metadata": {
        "id": "WopaZu42E4SN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 6: Convert Audio Data to Model-Ready Format**"
      ],
      "metadata": {
        "id": "GvCont7OE9gV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(audio_list):\n",
        "    inputs = feature_extractor(audio_list, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
        "    return inputs.input_values\n",
        "\n",
        "# Convert data\n",
        "X_train_tensors = preprocess_data(X_train)\n",
        "X_val_tensors = preprocess_data(X_val)\n",
        "X_test_tensors = preprocess_data(X_test)\n"
      ],
      "metadata": {
        "id": "XGb0R7_rFG5S"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Purpose**: Converts raw audio into tensor representations suitable for deep learning models."
      ],
      "metadata": {
        "id": "cWpEHAxjFH_J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 7: Load Wav2Vec 2.0 Model for Fine-Tuning**"
      ],
      "metadata": {
        "id": "T3oLVWeVFPvO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import Wav2Vec2ForSequenceClassification\n",
        "\n",
        "# Load the pre-trained model with 2 output labels (Fake/Real)\n",
        "model = Wav2Vec2ForSequenceClassification.from_pretrained(\n",
        "    \"facebook/wav2vec2-large-xlsr-53\",\n",
        "    num_labels=2\n",
        ")\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1249,
          "referenced_widgets": [
            "cb3bf80fe3d646cebc70a0ec69ff82eb",
            "3cd467769a23412989e8a6ec62772a60",
            "60f7e78691db4164a2db817fa6c9a4c5",
            "f9c7f08d4a8b488bbc395e9976d73734",
            "af6b83af31f54ad3afa1a18c0075954e",
            "0e8dc70f3a984bf79a53935d7dbad8bf",
            "febe4b5e14234ef2ae882bb09112a0d0",
            "87e441094d184016bbccb37f96443d7b",
            "3e4f24cd6ac242188849487f313da8a9",
            "3b0e622e471f420dbf73a703fdaf401d",
            "528f95bb11624637bcdd68e0c7e0a947"
          ]
        },
        "id": "2Juu3Y8kFaMR",
        "outputId": "b955dc66-0670-40f3-864a-ce7a2ecdd0af"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/1.27G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cb3bf80fe3d646cebc70a0ec69ff82eb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-large-xlsr-53 and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Wav2Vec2ForSequenceClassification(\n",
              "  (wav2vec2): Wav2Vec2Model(\n",
              "    (feature_extractor): Wav2Vec2FeatureEncoder(\n",
              "      (conv_layers): ModuleList(\n",
              "        (0): Wav2Vec2LayerNormConvLayer(\n",
              "          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,))\n",
              "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation): GELUActivation()\n",
              "        )\n",
              "        (1-4): 4 x Wav2Vec2LayerNormConvLayer(\n",
              "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n",
              "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation): GELUActivation()\n",
              "        )\n",
              "        (5-6): 2 x Wav2Vec2LayerNormConvLayer(\n",
              "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,))\n",
              "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation): GELUActivation()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (feature_projection): Wav2Vec2FeatureProjection(\n",
              "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (projection): Linear(in_features=512, out_features=1024, bias=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): Wav2Vec2EncoderStableLayerNorm(\n",
              "      (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
              "        (conv): ParametrizedConv1d(\n",
              "          1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
              "          (parametrizations): ModuleDict(\n",
              "            (weight): ParametrizationList(\n",
              "              (0): _WeightNorm()\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (padding): Wav2Vec2SamePadLayer()\n",
              "        (activation): GELUActivation()\n",
              "      )\n",
              "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (layers): ModuleList(\n",
              "        (0-23): 24 x Wav2Vec2EncoderLayerStableLayerNorm(\n",
              "          (attention): Wav2Vec2SdpaAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (feed_forward): Wav2Vec2FeedForward(\n",
              "            (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
              "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (projector): Linear(in_features=1024, out_features=256, bias=True)\n",
              "  (classifier): Linear(in_features=256, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Purpose**: Loads Wav2Vec 2.0 for classification and moves it to GPU for faster training"
      ],
      "metadata": {
        "id": "mt6Rx6VYFbbB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "** Step 8: Prepare Data for Training**"
      ],
      "metadata": {
        "id": "s3MBGZcuFgfB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.optim import Adam\n",
        "\n",
        "# Convert data into TensorDataset\n",
        "train_dataset = TensorDataset(torch.tensor(X_train_tensors).squeeze(), torch.tensor(y_train))\n",
        "val_dataset = TensorDataset(torch.tensor(X_val_tensors).squeeze(), torch.tensor(y_val))\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
        "\n",
        "# Define optimizer and loss function\n",
        "optimizer = Adam(model.parameters(), lr=1e-5)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "YVgPTYpbFoLY",
        "outputId": "290a6949-12cf-4611-d4af-51c2b1e64f8e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-213fa1e816c5>:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_dataset = TensorDataset(torch.tensor(X_train_tensors).squeeze(), torch.tensor(y_train))\n",
            "<ipython-input-15-213fa1e816c5>:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_dataset = TensorDataset(torch.tensor(X_val_tensors).squeeze(), torch.tensor(y_val))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Purpose:** Converts data into PyTorch DataLoaders for batch processing."
      ],
      "metadata": {
        "id": "VIAI8BjoFpb5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 9: Train the Model**"
      ],
      "metadata": {
        "id": "xpUM-kdTFuq5"
      }
    },
    {
      "source": [
        "import torch\n",
        "from transformers import Wav2Vec2ForSequenceClassification\n",
        "\n",
        "# Load the pre-trained model with 2 output labels (Fake/Real)\n",
        "model = Wav2Vec2ForSequenceClassification.from_pretrained(\n",
        "    \"facebook/wav2vec2-large-xlsr-53\",\n",
        "    num_labels=2\n",
        ")\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "epochs = 3\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()  # Ensure the model is in training mode\n",
        "    total_loss = 0\n",
        "    # Simulated input\n",
        "    inputs = torch.randn(1, 16000)  # Example input (modify for actual use)\n",
        "    labels = torch.tensor([1]).unsqueeze(0)  # Example label\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(inputs, labels=labels)\n",
        "    loss = outputs.loss\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {total_loss}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "p_MoOZOFKS1w",
        "outputId": "73c5ced8-8578-4c1f-b320-1313867833b7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-large-xlsr-53 and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.7229881882667542\n",
            "Epoch 2, Loss: 0.6789950728416443\n",
            "Epoch 3, Loss: 0.7078856229782104\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "** Purpose:** Fine-tunes Wav2Vec 2.0 on deepfake vs. real speech data."
      ],
      "metadata": {
        "id": "-9h-MjNrF4Sd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 10: Evaluate Model Performance**"
      ],
      "metadata": {
        "id": "hLaed0IsF-p1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Convert Test Data to Tensors**"
      ],
      "metadata": {
        "id": "3kcsRk_oOgay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Convert X_test and y_test (NumPy arrays) to PyTorch tensors\n",
        "X_test_tensors = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_tensors = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# Create TensorDataset for test data\n",
        "test_dataset = TensorDataset(X_test_tensors, y_test_tensors)\n",
        "\n",
        "# Define DataLoader for test set\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "R4kl-lYDOiwe",
        "outputId": "b8d8b41d-84f3-400a-b12a-bced85407c53"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-913a85c9c726>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n",
            "  X_test_tensors = torch.tensor(X_test, dtype=torch.float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Ensure model is in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Initialize lists to store predictions and true labels\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "# Run inference without tracking gradients\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:  # test_loader is now correctly defined\n",
        "        inputs, labels = batch  # Extract inputs and labels\n",
        "        inputs = inputs.to(device)  # Move inputs to GPU/CPU\n",
        "        labels = labels.to(device)  # Move labels to GPU/CPU\n",
        "\n",
        "        # Get model predictions\n",
        "        outputs = model(inputs).logits\n",
        "        preds = torch.argmax(outputs, dim=1)  # Get class predictions\n",
        "\n",
        "        # Store predictions and labels\n",
        "        all_preds.extend(preds.cpu().numpy())  # Convert to NumPy for metrics\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Convert lists to NumPy arrays\n",
        "all_preds = np.array(all_preds)\n",
        "all_labels = np.array(all_labels)\n",
        "\n",
        "# Calculate Accuracy\n",
        "accuracy = accuracy_score(all_labels, all_preds)\n",
        "print(f\"🔹 Model Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Print Classification Report\n",
        "print(\"\\n🔹 Classification Report:\")\n",
        "print(classification_report(all_labels, all_preds))\n",
        "\n",
        "# Confusion Matrix\n",
        "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "# Plot Confusion Matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Real\", \"Fake\"], yticklabels=[\"Real\", \"Fake\"])\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 782
        },
        "id": "QEliTTcBRbbb",
        "outputId": "214576af-07da-4184-eb98-c2121fe0124a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 Model Accuracy: 0.5000\n",
            "\n",
            "🔹 Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      1.00      0.67       544\n",
            "           1       0.50      0.00      0.01       544\n",
            "\n",
            "    accuracy                           0.50      1088\n",
            "   macro avg       0.50      0.50      0.34      1088\n",
            "weighted avg       0.50      0.50      0.34      1088\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "** Purpose: **Measures model accuracy and classification report for real vs. deepfake speech."
      ],
      "metadata": {
        "id": "fXg7XKhmGH6F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 11: Save & Export the Model**"
      ],
      "metadata": {
        "id": "eCJEgZoQGMFd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"/content/drive/My Drive/Wav2Vec2_Deepfake_Detector\")\n"
      ],
      "metadata": {
        "id": "xBsRBGrWGVGL"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Purpose**: Saves the trained model for future inference."
      ],
      "metadata": {
        "id": "5hldDmNrGYyx"
      }
    }
  ]
}